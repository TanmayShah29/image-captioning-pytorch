# Image Captioning with PyTorch

A beginner-friendly implementation of image captioning using CNN+LSTM architecture with transfer learning on the Flickr8k dataset.

## ğŸ“‹ Project Overview

### What is Image Captioning?

Image captioning is the task of automatically generating textual descriptions for images. It combines:
- **Computer Vision**: Understanding image content
- **Natural Language Processing**: Generating descriptive text

### Real-World Applications

- ğŸ¦¾ **Assistive Technology**: Helping visually impaired users understand images
- ğŸ“± **Social Media**: Automatic alt-text generation for accessibility
- ğŸ” **Search Engines**: Content indexing and image search
- ğŸ¥ **Medical Imaging**: Automated report generation

### Architecture: CNN + LSTM

This project uses a two-part architecture:

1. **CNN Encoder (ResNet50)**
   - Pre-trained on ImageNet (1.2M images)
   - Extracts visual features from images
   - **Frozen weights** (transfer learning)

2. **LSTM Decoder**
   - Generates captions word-by-word
   - **Trained from scratch** on Flickr8k
   - Uses teacher forcing during training

> **Important for Viva**: You trained the LSTM decoder from scratch. The CNN uses transfer learning (pre-trained weights). This is a standard and academically valid approach in deep learning.

---

## ğŸ—‚ï¸ Project Structure

```
image_captioning/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Flickr8k_Dataset/          # Images (8,000 images)
â”‚   â””â”€â”€ Flickr8k_text/              # Captions text files
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ encoder.py                  # CNN feature extractor (ResNet50)
â”‚   â””â”€â”€ decoder.py                  # LSTM caption generator
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ vocabulary.py               # Vocabulary builder
â”‚   â””â”€â”€ dataset.py                  # Dataset loader
â”‚
â”œâ”€â”€ train.py                        # Training script
â”œâ”€â”€ inference.py                    # Caption generation script
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README.md                       # This file
```

---

## ğŸ“¦ Dataset: Flickr8k

### Why Flickr8k?

- âœ… **Beginner-friendly**: Only 8,000 images (manageable size)
- âœ… **CPU/Colab compatible**: Can train without expensive GPU
- âœ… **Quality captions**: 5 captions per image
- âœ… **Academic standard**: Widely used in research

### Download Dataset

1. **Download from Kaggle**:
   ```bash
   # Visit: https://www.kaggle.com/datasets/adityajn105/flickr8k
   # Download both:
   # - Flickr8k_Dataset.zip (images)
   # - Flickr8k_text.zip (captions)
   ```

2. **Extract to project**:
   ```bash
   # Extract images to:
   data/Flickr8k_Dataset/
   
   # Extract captions to:
   data/Flickr8k_text/
   ```

3. **Verify structure**:
   ```
   data/
   â”œâ”€â”€ Flickr8k_Dataset/
   â”‚   â”œâ”€â”€ 1000268201_693b08cb0e.jpg
   â”‚   â”œâ”€â”€ 1001773457_577c3a7d70.jpg
   â”‚   â””â”€â”€ ...
   â””â”€â”€ Flickr8k_text/
       â”œâ”€â”€ Flickr8k.token.txt
       â”œâ”€â”€ Flickr_8k.trainImages.txt
       â””â”€â”€ Flickr_8k.testImages.txt
   ```

---

## ğŸš€ Installation

### Prerequisites

- Python 3.7 or higher
- pip package manager

### Install Dependencies

```bash
# Clone or download this project
cd image_captioning

# Install required packages
pip install -r requirements.txt
```

### Dependencies

- `torch`: PyTorch deep learning framework
- `torchvision`: Pre-trained models and image transformations
- `Pillow`: Image processing
- `numpy`: Numerical operations
- `matplotlib`: Visualization
- `tqdm`: Progress bars

---

## ğŸ“ Training

### Quick Start

```bash
python train.py
```

### Training Configuration

Edit `train.py` to customize:

```python
config = {
    "data_dir": "data",
    "num_epochs": 10,          # Increase for better results (20-30)
    "batch_size": 32,          # Reduce if out of memory (16 or 8)
    "learning_rate": 0.001,
    "embed_size": 256,
    "hidden_size": 512,
    "num_layers": 1,
    "save_dir": "saved_models"
}
```

### Training Process

The training script will:

1. âœ… Load Flickr8k dataset
2. âœ… Build vocabulary from captions
3. âœ… Initialize encoder (frozen ResNet50) and decoder (trainable LSTM)
4. âœ… Train decoder using teacher forcing
5. âœ… Save model checkpoints after each epoch
6. âœ… Generate training loss plot

### Expected Training Time

- **CPU**: ~30 minutes per epoch
- **GPU**: ~5 minutes per epoch
- **Google Colab (free GPU)**: ~5 minutes per epoch

### Output Files

After training, you'll find:

```
saved_models/
â”œâ”€â”€ final_model.pth           # Trained model
â”œâ”€â”€ vocabulary.pkl            # Vocabulary object
â”œâ”€â”€ training_loss.png         # Loss plot
â””â”€â”€ checkpoint_epoch_*.pth    # Checkpoints
```

---

## ğŸ”® Inference (Testing)

### Generate Caption for Single Image

```bash
python inference.py --image path/to/your/image.jpg
```

### Save Result

```bash
python inference.py --image path/to/image.jpg --save result.png
```

### Use Specific Checkpoint

```bash
python inference.py --image test.jpg --model saved_models/checkpoint_epoch_5.pth
```

### Example

```bash
python inference.py --image data/Flickr8k_Dataset/1000268201_693b08cb0e.jpg
```

Output:
```
Using device: cpu

Loading model and vocabulary...
Model loaded from epoch 10
Vocabulary size: 2538

Generating caption for: data/Flickr8k_Dataset/1000268201_693b08cb0e.jpg

Generated Caption: a child in a pink dress is climbing up a set of stairs
```

---

## ğŸ¯ Key Concepts Explained

### 1. Transfer Learning

**What**: Using a pre-trained model (ResNet50) trained on ImageNet

**Why**: 
- Saves training time
- Leverages learned visual features
- Works well with limited data

**In this project**:
- CNN encoder is pre-trained and frozen
- Only LSTM decoder is trained

### 2. Teacher Forcing

**What**: During training, feed the actual previous word (not predicted)

**Why**: Helps model learn faster and more stably

**Example**:
```
Actual caption: "a dog running"
Step 1: Input <start> â†’ Predict "a"
Step 2: Input "a" (actual) â†’ Predict "dog"
Step 3: Input "dog" (actual) â†’ Predict "running"
```

### 3. Greedy Decoding

**What**: At each step, pick word with highest probability

**Why**: Simple and fast for inference

**Example**:
```
Step 1: Input <start> â†’ Pick "a" (highest prob)
Step 2: Input "a" â†’ Pick "dog" (highest prob)
Step 3: Input "dog" â†’ Pick "running" (highest prob)
Step 4: Input "running" â†’ Pick <end> (stop)
```

### 4. Special Tokens

- `<start>`: Beginning of caption
- `<end>`: End of caption
- `<pad>`: Padding for shorter captions
- `<unk>`: Unknown words (not in vocabulary)

### 5. Vocabulary Building

**Process**:
1. Tokenize all captions (split into words)
2. Count word frequencies
3. Keep words appearing â‰¥ 5 times
4. Map words to numerical indices

**Why**: Neural networks work with numbers, not text

---

## ğŸ“Š Model Architecture

### Encoder (CNN)

```
Input Image (224Ã—224Ã—3)
    â†“
ResNet50 (pre-trained, frozen)
    â†“
Remove final classification layer
    â†“
Linear projection
    â†“
Feature Vector (256 dimensions)
```

### Decoder (LSTM)

```
Image Features (256)
    â†“
Word Embeddings (vocab_size â†’ 256)
    â†“
LSTM (256 â†’ 512)
    â†“
Fully Connected (512 â†’ vocab_size)
    â†“
Softmax â†’ Predicted Word
```

### Parameters

- **Encoder**: ~23M parameters (frozen)
- **Decoder**: ~5M parameters (trainable)
- **Total trainable**: ~5M parameters

---

## ğŸ¤ Viva Preparation

See [VIVA_GUIDE.md](VIVA_GUIDE.md) for:
- Common viva questions with answers
- One-minute project explanation
- Block diagram description
- Technical concepts explained simply

---

## ğŸ› Troubleshooting

### Issue: Out of Memory

**Solution**: Reduce batch size in `train.py`
```python
"batch_size": 16  # or 8
```

### Issue: Dataset not found

**Solution**: Verify dataset structure
```bash
ls data/Flickr8k_Dataset/
ls data/Flickr8k_text/
```

### Issue: Slow training

**Solution**: Use Google Colab for free GPU
1. Upload project to Google Drive
2. Open Colab notebook
3. Mount Drive and run training

### Issue: Poor caption quality

**Solution**: Train for more epochs
```python
"num_epochs": 20  # or 30
```

---

## ğŸ“š Learning Resources

### Understanding the Code

Each file has extensive comments explaining:
- What each function does
- Why it's needed
- Input/output shapes
- Key concepts

### Recommended Reading

1. **CNN**: [CS231n Convolutional Neural Networks](http://cs231n.github.io/)
2. **LSTM**: [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
3. **Transfer Learning**: [CS231n Transfer Learning](http://cs231n.github.io/transfer-learning/)

---

## ğŸ“ Academic Honesty

### For Viva Defense

âœ… **Correct statements**:
- "I used transfer learning for the CNN encoder"
- "I trained the LSTM decoder from scratch"
- "This is a standard approach in deep learning"
- "The encoder extracts features, the decoder generates captions"

âŒ **Avoid saying**:
- "I trained the entire model from scratch"
- "I created ResNet50"

### What You Actually Trained

- âœ… LSTM decoder (~5M parameters)
- âœ… Word embeddings
- âœ… Linear projection layers
- âŒ CNN encoder (pre-trained, frozen)

---

## ğŸ“ License

This project is for educational purposes. Feel free to use for college projects and learning.

---

## ğŸ™ Acknowledgments

- **Dataset**: Flickr8k by Hodosh et al.
- **Framework**: PyTorch
- **Pre-trained Model**: ResNet50 from torchvision

---

## ğŸ“§ Support

For questions or issues:
1. Check [VIVA_GUIDE.md](VIVA_GUIDE.md)
2. Review code comments
3. Consult PyTorch documentation

---

**Good luck with your project and viva! ğŸ“**
