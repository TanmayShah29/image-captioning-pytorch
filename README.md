# Image Captioning with PyTorch

End-to-end image captioning on **Flickr8k** using a CNNâ€“LSTM architecture (ResNet50 encoder + LSTM decoder).

## âœ¨ Key Features

- **Bulletproof dataset handling** â€” any Flickr8k format auto-detected and normalized
- **Beam search inference** for higher-quality captions
- **BLEU-1â†’4 evaluation** every 3 epochs
- **Early stopping**, LR scheduling, mixed-precision training
- **Full checkpoint save/resume** â€” safe for Colab disconnects
- **YAML config** with CLI overrides

---

## ğŸš€ Quick Start

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

### 2. Get the Flickr8k dataset

Download from [Kaggle](https://www.kaggle.com/datasets/adityajn105/flickr8k) or any other source and place files under `data/`:

```
data/
â”œâ”€â”€ Flickr8k_Dataset/   â† or Images/, or any folder with .jpg files
â”œâ”€â”€ Flickr8k.token.txt  â† or captions.csv, or any caption format
```

> **Any layout works.** The pipeline auto-detects and normalizes everything.

### 3. Verify dataset (optional but recommended)

```bash
python test_dataset_loading.py
```

### 4. Train

```bash
python train.py
```

### 5. Generate captions

```bash
python inference.py --image path/to/image.jpg --beam_size 3
```

---

## ğŸ”’ Dataset Auto-Normalization

**Training will refuse to start if the dataset is broken.**

When you run `train.py` (or `verify.py` or `test_dataset_loading.py`), the preparation pipeline automatically:

1. **Finds images** in any subfolder under `data/` (Kaggle layout, manual layout, nested folders â€” all supported)
2. **Finds captions** in any format:
   - `image.jpg#0<TAB>caption` (Flickr8k token format)
   - `image,caption` (CSV with or without header)
   - `image<TAB>caption` (plain TSV)
3. **Copies images** into `data/images/` (flat, one directory)
4. **Writes clean** `data/captions.txt` (tab-separated, no headers, no `#0` suffixes)
5. **Cross-validates** that every caption points to an existing image
6. **Crashes with a clear error** if anything is wrong â€” telling you WHAT failed, WHY, and HOW to fix it

After preparation, all code reads **only** the canonical format:

```
data/
â”œâ”€â”€ images/           â† all .jpg files here
â””â”€â”€ captions.txt      â† image_name<TAB>caption
```

> [!NOTE]
> Once prepared, re-running skips the normalization step (fast path).

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ config.yaml              â† hyperparameters (YAML)
â”œâ”€â”€ train.py                 â† training with val loop, early stopping, AMP
â”œâ”€â”€ inference.py             â† caption generation (greedy + beam search)
â”œâ”€â”€ evaluate.py              â† BLEU-1â†’4 scoring
â”œâ”€â”€ verify.py                â† 6-check end-to-end verification
â”œâ”€â”€ test_dataset_loading.py  â† dataset dry-run test
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ encoder.py           â† ResNet50 feature extractor (frozen)
â”‚   â””â”€â”€ decoder.py           â† LSTM + beam search
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ prepare_dataset.py   â† dataset auto-detection & normalization
â”‚   â”œâ”€â”€ dataset.py           â† PyTorch Dataset (canonical format only)
â”‚   â””â”€â”€ vocabulary.py        â† word â†” index mapping
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Configuration

All hyperparameters live in `config.yaml` and can be overridden via CLI:

```bash
# Override any parameter
python train.py --num_epochs 20 --learning_rate 0.0005 --batch_size 64

# Resume interrupted training
python train.py --resume saved_models/checkpoint_latest.pth
```

| Parameter | Default | Description |
|-----------|---------|-------------|
| `num_epochs` | 10 | Training epochs |
| `batch_size` | 32 | Batch size |
| `learning_rate` | 0.001 | Initial learning rate |
| `embed_size` | 256 | Embedding dimension |
| `hidden_size` | 512 | LSTM hidden dimension |
| `freq_threshold` | 5 | Min word frequency for vocabulary |
| `early_stop_patience` | 5 | Epochs without improvement before stopping |
| `use_amp` | true | Mixed-precision training |

---

## ğŸ“Š Evaluation

BLEU scores are computed automatically during training. For standalone evaluation:

```python
from evaluate import evaluate_bleu
bleu = evaluate_bleu(encoder, decoder, val_dataset, vocab, device, max_samples=500)
```

---

## ğŸ§ª Verification

```bash
# Full pipeline check (6 automated tests)
python verify.py

# Dataset-only dry run (5 samples)
python test_dataset_loading.py
```

---

## License

MIT
